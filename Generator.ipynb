{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as func\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCRIM_DIM = (128, 128)\n",
    "\n",
    "SAMPLE_RATE = 44000\n",
    "TEMP_RES_SPEC = 1000\n",
    "FREQ_RES_SPEC = 800\n",
    "CONV_FEATURES = 25\n",
    "TEMP_RES_ENC = 80\n",
    "EMB_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyzer(nn.Module):\n",
    "    \n",
    "    def __init__(self, parser, sample_rate, temp_res_spec, freq_res_spec, \n",
    "                conv_features, temp_res_enc, emb_size):\n",
    "        \"\"\"\n",
    "        sample_rate: sample rate of the audio input\n",
    "        \n",
    "        temp_res_spec: temporal resolution (in slices per second) of the \n",
    "            spectrogram that is performed on the audio clip\n",
    "        \n",
    "        freq_res_spec: frequency resolution (in total number of slices)\n",
    "            of the spectrogram that is performed on the audio clip\n",
    "            \n",
    "        temp_res_enc: the temporal resolution (again in slices / sec) of \n",
    "            of the encoded signal that is returned from this object\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.sample_rate = sample_rate\n",
    "        self.temp_res_spec = temp_res_spec\n",
    "        self.freq_res_spec = freq_res_spec\n",
    "        self.temp_res_enc = temp_res_enc\n",
    "        self.conv_features = conv_features\n",
    "        self.emb_size = emb_size\n",
    "        \n",
    "        self.freq_res_trimmed = round(self.freq_res_spec * 0.35)\n",
    "        \n",
    "        self.conv_width = round(temp_res_spec / temp_res_enc)\n",
    "        \n",
    "        conv_height1 = round(self.freq_res_trimmed / 3)\n",
    "        kernel_size1 = (self.conv_width, conv_height1)\n",
    "        padding_size1 = (0, conv_height1 - 1)\n",
    "        stride_size1 = (self.conv_width, round(conv_height1 / 3))\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels=conv_features,\n",
    "                                kernel_size=kernel_size1, \n",
    "                                padding=padding_size1,\n",
    "                                stride=stride_size1)\n",
    "        \n",
    "        conv_height2 = round(self.freq_res_trimmed / 13)\n",
    "        kernel_size2 = (self.conv_width, conv_height2)\n",
    "        padding_size2 = (0, conv_height2 - 1)\n",
    "        stride_size2 = (self.conv_width, round(conv_height2 / 13))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=1, out_channels=conv_features,\n",
    "                                kernel_size=kernel_size2,\n",
    "                                padding=padding_size2,\n",
    "                                stride=stride_size2)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            self.out_size1 = self.conv1(torch.ones(1, 1, self.temp_res_spec,\n",
    "                                            self.freq_res_trimmed)).size()\n",
    "            \n",
    "            self.out_size2 = self.conv2(torch.ones(1, 1, self.temp_res_spec, \n",
    "                                              self.freq_res_trimmed)).size()\n",
    "        \n",
    "        linear_input_size = self.out_size1[3] + self.out_size2[3]\n",
    "        self.linear1 = nn.Linear(linear_input_size, emb_size)\n",
    "        \n",
    "        self.linear2 = nn.Linear(self.conv_features, 1)\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=emb_size, hidden_size=int(emb_size / 2), bidirectional=True)\n",
    "        \n",
    "        self.cel = nn.LSTMCell(input_size=emb_size, hidden_size=int(emb_size / 2))\n",
    "        \n",
    "        self.parser = parser\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x, single_word=False):\n",
    "        window_size = self.freq_res_spec\n",
    "        naud = x.size()[1]\n",
    "        nsec = (naud / self.sample_rate)\n",
    "        nspec = nsec * self.temp_res_spec\n",
    "        hop = round(naud / nspec)\n",
    "        \n",
    "        spec = func.spectrogram(\n",
    "                sig=x, \n",
    "                pad=0, window=torch.bartlett_window(window_size), \n",
    "                n_fft=round(window_size * 2), hop=hop, \n",
    "                ws=window_size, power=2, normalize=False)\n",
    "        \n",
    "        spec = torch.sqrt(spec)\n",
    "        spec = spec.unsqueeze(0)\n",
    "        \n",
    "        out1 = self.conv1(spec)\n",
    "        out2 = self.conv2(spec)\n",
    "        \n",
    "        out3 = torch.cat((out1.squeeze(0), out2.squeeze(0)), dim=2)\n",
    "        out3 = torch.transpose(out3, 0, 1)\n",
    "        \n",
    "        out4 = self.linear1(out3)\n",
    "        \n",
    "        out5 = torch.transpose(out4, 1, 2)\n",
    "        out5 = self.linear2(out5)\n",
    "        out5 = torch.transpose(out5, 1, 2)\n",
    "        \n",
    "        out6 = self.gru(out5)\n",
    "        \n",
    "        words = torch.empty((1, self.emb_size, 0))\n",
    "        seq_tags = torch.empty((4, 1))\n",
    "        \n",
    "        cel_state = torch.empty((1, int(self.emb_size / 2)))\n",
    "        nn.init.xavier_uniform_(cel_state)\n",
    "        \n",
    "        hid_state = torch.empty((1, int(self.emb_size / 2)))\n",
    "        nn.init.xavier_normal_(hid_state)\n",
    "        \n",
    "        bit = 0\n",
    "        last_bit = 0\n",
    "        new_word = False\n",
    "\n",
    "        for s in out6[0]:\n",
    "            hid_state, cel_state = self.cel(s, (hid_state, cel_state))\n",
    "            \n",
    "            cur_state = torch.cat((cel_state, hid_state), dim=1)\n",
    "            \n",
    "            res = self.linear3(cur_state)\n",
    "            res = nn.functional.softmax(res, 1)\n",
    "            \n",
    "            seq_tags = torch.cat((seq_tags, res.transpose(0, 1)), 1)\n",
    "            \n",
    "            last_bit = bit\n",
    "            bit = torch.argmax(res)\n",
    "            \n",
    "            if(bit != 0 and last_bit == 0):\n",
    "                new_word = True\n",
    "                \n",
    "            if(bit == 1 and last_bit == 3):\n",
    "                new_word = True\n",
    "            \n",
    "            if(new_word and not single_word):\n",
    "                new_word = False\n",
    "                words = torch.cat((words, cur_state.unsqueeze(2)), dim=2)\n",
    "                cel_state = torch.empty((1, int(self.emb_size / 2)))\n",
    "                nn.init.xavier_normal_(cel_state)\n",
    "        \n",
    "        if(single_word):\n",
    "            words = torch.cat((words, cur_state.unsqueeze(2)), dim=2)\n",
    "        \n",
    "        return (seq_tags, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechNet():\n",
    "    def __init__(self, high_grit, low_grit, parser, recognizer):\n",
    "        \"\"\"\n",
    "        high_grit: reversible conv layer to go from feature-scale\n",
    "                    to audio-scale resolution and vice versa\n",
    "                    \n",
    "        low_grit: reversible conv layer to go from feature-scale\n",
    "                    to low feature-scale resolution for RNN \n",
    "                    processing\n",
    "        \n",
    "        parser: reversible module to go from low feature-scale \n",
    "                    resolution to discrete word encodings at much\n",
    "                    lower temporal resolution (output still time-dep.)\n",
    "                    \n",
    "        recognizer: reversible module to go from word encodings\n",
    "                    of variable time dimension to words in the\n",
    "                    vocabulary by way of linear layers.\n",
    "        \"\"\"\n",
    "\n",
    "        self.high_grit = high_grit\n",
    "        self.low_grit = low_grit\n",
    "        self.parser = parser\n",
    "        self.recognizer = recognizer\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def text_to_spec(self, x):\n",
    "        x = self.recognizer(x)\n",
    "        x = self.parser(x)\n",
    "        x = self.low_grit(x)\n",
    "        x = self.high_grit(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def spec_to_text(self, x):\n",
    "        x = self.high_grit(x)\n",
    "        x = self.low_grit(x)\n",
    "        x = self.parser(x)\n",
    "        x = self.recognizer(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def text_to_rec(self, x):\n",
    "        x = self.recognizer(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def spec_to_rec(self, x):\n",
    "        x = self.high_grit(x)\n",
    "        x = self.low_grit(x)\n",
    "        x = self.parser(x)\n",
    "        x = self.recognizer(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def text_to_seq(self, x):\n",
    "        x = self.recognizer(x)\n",
    "        x = self.parser(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def spec_to_seq(self, x):\n",
    "        x = self.high_grit(x)\n",
    "        x = self.low_grit(x)\n",
    "        x = self.parser(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def text_to_low(self, x):\n",
    "        x = self.recognizer(x)\n",
    "        x = self.parser(x)\n",
    "        x = self.low_grit(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def spec_to_low(self, x):\n",
    "        x = self.high_grit(x)\n",
    "        x = self.low_grit(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def run_train(self, dataset, num_epochs=15, mon_interval=100):\n",
    "        \"\"\"\n",
    "        dataset: an iterable dataset\n",
    "        num_epochs: how many epochs to do the training\n",
    "        mon_interval: how often to output monitoring info\n",
    "        \"\"\"\n",
    "        \n",
    "        for i, (sounds, labels) in enumerate(dataset):\n",
    "            \n",
    "            # Set up this batch of data\n",
    "            spec = self.sound_to_spec(sounds)\n",
    "            text = self.labels_to_text(labels)\n",
    "            \n",
    "            optimizer = optim.Adam(self.parameters())\n",
    "            \n",
    "            # Train the parser\n",
    "            optimizer.zero_grad()\n",
    "            self.parser.train(True)\n",
    "            seq1 = self.text_to_seq(text)\n",
    "            seq2 = self.spec_to_seq(sounds)\n",
    "            loss1 = nn.MarginLoss(seq1, seq2)\n",
    "            loss1.backward()\n",
    "            optimizer.step()\n",
    "            self.parser.train(False)\n",
    "            \n",
    "            # Train the recognizer            \n",
    "            optimizer.zero_grad()\n",
    "            self.recognizer.train(True)\n",
    "            rec1 = self.text_to_rec(text)\n",
    "            rec2 = self.spec_to_rec(spec)\n",
    "            loss2 = nn.MSELoss(rec1, rec2)\n",
    "            loss2.backward()\n",
    "            optimizer.step()\n",
    "            self.recognizer.train(False)\n",
    "            \n",
    "            # Train the low-grit conv layer\n",
    "            optimizer.zero_grad()\n",
    "            lg1 = self.spec_to_low(spec)\n",
    "            lg2 = self.text_to_low(text)\n",
    "            loss3 = nn.MSELoss(lg1, lg2)\n",
    "            loss3.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Train the high-grit conv layer\n",
    "            optimizer.zero_grad()\n",
    "            hg1 = self.spec_to_high(spec)\n",
    "            hg2 = self.text_to_high(text)\n",
    "            loss4 = nn.MSELoss(hg1, hg2)\n",
    "            loss4.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def forward(self, x, c = None):\n",
    "        if (c is not None):\n",
    "            return self.listen(x, c)\n",
    "        \n",
    "        else:\n",
    "            return self.speak(x, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
